import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler,LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier,VotingClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

data = pd.read_csv('hypothyroid.csv')  
data

data.describe()

data.columns

data.replace('?', np.nan, inplace=True)
print("Replaced '?' with NaN. Missing values per column:\n", data.isnull().sum())

data = data.drop(columns=['TBG'])
print("Dropped 'TBG' column. New shape:", data.shape)

label_encoders = {}
for column in data.select_dtypes(include=['object']).columns:
    le = LabelEncoder()
    data[column] = data[column].fillna(data[column].mode()[0])  
    data[column] = le.fit_transform(data[column])
    label_encoders[column] = le
    print(f"Encoded column '{column}' with {len(le.classes_)} unique values.")

for column in data.select_dtypes(include=['float64']).columns:
    data[column] = data[column].fillna(data[column].median())
print("Filled missing numerical values with median. Missing values now:\n", data.isnull().sum())

print("Splitting the dataset")
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print("Training set shape:", X_train.shape, "Test set shape:", X_test.shape)

print(" Standardizing features")
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
print("Features standardized. Sample of X_train:\n", X_train[:5])

classifiers = {
    'Decision Tree': DecisionTreeClassifier(random_state=42),
    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),
    'Random Forest': RandomForestClassifier(random_state=42),
    'SVM': SVC(random_state=42),
    'k-NN': KNeighborsClassifier(),
    'Naive Bayes': GaussianNB(),
}

ensemble = VotingClassifier(estimators=[
    ('dt', classifiers['Decision Tree']),
    ('lr', classifiers['Logistic Regression']),
    ('rf', classifiers['Random Forest']),
    ('svm', classifiers['SVM']),
    ('knn', classifiers['k-NN']),
    ('nb', classifiers['Naive Bayes'])
], voting='hard')
classifiers['Ensemble'] = ensemble
print("Classifiers initialized:", list(classifiers.keys()))

results = {}
for name, clf in classifiers.items():
    print(f"\nTraining {name}...")
    if name == 'K-Means':
        clf.fit(X_train)
        y_pred = clf.predict(X_test)
        y_pred = [1 if x == 1 else 0 for x in y_pred]  
    else:
        clf.fit(X_train, y_train)
        y_pred = clf.predict(X_test)
    
    # Calculate metrics
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='binary')
    recall = recall_score(y_test, y_pred, average='binary')
    f1 = f1_score(y_test, y_pred, average='binary')
    
    results[name] = {
        'Accuracy': accuracy,
        'Precision': precision,
        'Recall': recall,
        'F1-Score': f1
    }
    print(f"{name} Results:")
    print(f"Accuracy: {accuracy:.2f}")
    print(f"Precision: {precision:.2f}")
    print(f"Recall: {recall:.2f}")
    print(f"F1-Score: {f1:.2f}")


print("\nStep 8: Visualizing accuracy comparison...")
models = list(results.keys())
accuracies = [results[model]['Accuracy'] for model in models]

plt.figure(figsize=(10, 6))
plt.bar(models, accuracies, color='skyblue')
plt.title('Comparison of Model Accuracies')
plt.xlabel('Models')
plt.ylabel('Accuracy')
plt.xticks(rotation=45)
plt.ylim(0, 1)
plt.tight_layout()
plt.show()

print("\nComparison Table: Script Results vs Base Paper")
print("-" * 80)
print(f"{'Model':<20} {'Metric':<15} {'Script':<10} {'Base Paper':<10} {'Difference':<10}")
print("-" * 80)
for model in results.keys():
    for metric in ['Accuracy', 'Precision', 'Recall', 'F1-Score']:
        script_val = results[model][metric]
        base_val = base_paper_results.get(model, {}).get(metric, None)
        diff = script_val - base_val if base_val is not None else None
        
        base_val_str = f"{base_val:<10.2f}" if base_val is not None else f"{'N/A':<10}"
        diff_str = f"{diff:<10.2f}" if diff is not None else f"{'N/A':<10}"
        print(f"{model:<20} {metric:<15} {script_val:<10.2f} {base_val_str} {diff_str}")
print("-" * 80)

from scipy.stats import uniform, randint
param_distributions = {
    'Decision Tree': {'max_depth': [None] + list(range(5, 21, 5)), 'min_samples_split': randint(2, 11)},
    'Logistic Regression': {'C': uniform(0.1, 10), 'solver': ['lbfgs', 'liblinear']},
    'Random Forest': {'n_estimators': randint(50, 201), 'max_depth': [None] + list(range(10, 31, 10))},
    'SVM': {'C': uniform(0.1, 10), 'kernel': ['linear', 'rbf']},
    'k-NN': {'n_neighbors': randint(3, 10), 'weights': ['uniform', 'distance']},
    'Naive Bayes': {}
}

param_grids = {
    'Decision Tree': {'max_depth': [None, 10, 20], 'min_samples_split': [2, 5, 10]},
    'Logistic Regression': {'C': [0.1, 1, 10], 'solver': ['lbfgs', 'liblinear']},
    'Random Forest': {'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20]},
    'SVM': {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']},
    'k-NN': {'n_neighbors': [3, 5, 7], 'weights': ['uniform', 'distance']},
    'Naive Bayes': {}
}

def plot_correlation_heatmap(X_data, title):
    X_df = pd.DataFrame(X_data, columns=[f'Feature_{i}' for i in range(X_data.shape[1])])
    correlation_matrix = X_df.corr()
    plt.figure(figsize=(12, 8))
    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')
    plt.title(title)
    plt.show()


def plot_correlation_heatmap(X_data, title):
    X_df = pd.DataFrame(X_data, columns=[f'Feature_{i}' for i in range(X_data.shape[1])])
    correlation_matrix = X_df.corr()
    plt.figure(figsize=(12, 8))
    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')
    plt.title(title)
    plt.show()

# --- Task 1: Basic Hyperparameter Tuning (GridSearchCV) ---
print("\n Task 1: Basic Hyperparameter Tuning (GridSearchCV)")
results_task1_grid = {}
for name, clf in classifiers.items():
    print(f"\nTuning {name} with GridSearchCV...")
    if param_grids[name]:
        grid_search = GridSearchCV(clf, param_grids[name], cv=5, scoring='accuracy', n_jobs=-1)
        grid_search.fit(X_train, y_train)
        print(f"Best parameters: {grid_search.best_params_}")
        clf = grid_search.best_estimator_
    
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    results_task1_grid[name] = {
        'Accuracy': accuracy_score(y_test, y_pred),
        'Precision': precision_score(y_test, y_pred, average='binary'),
        'Recall': recall_score(y_test, y_pred, average='binary'),
        'F1-Score': f1_score(y_test, y_pred, average='binary')
    }
    print(f"{name} Results: Accuracy={results_task1_grid[name]['Accuracy']:.2f}")

ensemble_task1_grid = VotingClassifier(estimators=[(k, classifiers[k]) for k in classifiers], voting='hard')
ensemble_task1_grid.fit(X_train, y_train)
y_pred = ensemble_task1_grid.predict(X_test)
results_task1_grid['Ensemble'] = {
    'Accuracy': accuracy_score(y_test, y_pred),
    'Precision': precision_score(y_test, y_pred, average='binary'),
    'Recall': recall_score(y_test, y_pred, average='binary'),
    'F1-Score': f1_score(y_test, y_pred, average='binary')
}
print(f"Ensemble Results: Accuracy={results_task1_grid['Ensemble']['Accuracy']:.2f}")

# Task 1: Basic Hyperparameter Tuning (RandomizedSearchCV)
print("\n Task 1: Basic Hyperparameter Tuning (RandomizedSearchCV) ")
results_task1_random = {}
for name, clf in classifiers.items():
    print(f"\nTuning {name} with RandomizedSearchCV...")
    if param_distributions[name]:
        random_search = RandomizedSearchCV(clf, param_distributions[name], n_iter=10, cv=5, scoring='accuracy', n_jobs=-1, random_state=42)
        random_search.fit(X_train, y_train)
        print(f"Best parameters: {random_search.best_params_}")
        clf = random_search.best_estimator_
    
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    results_task1_random[name] = {
        'Accuracy': accuracy_score(y_test, y_pred),
        'Precision': precision_score(y_test, y_pred, average='binary'),
        'Recall': recall_score(y_test, y_pred, average='binary'),
        'F1-Score': f1_score(y_test, y_pred, average='binary')
    }
    print(f"{name} Results: Accuracy={results_task1_random[name]['Accuracy']:.2f}")

ensemble_task1_random = VotingClassifier(estimators=[(k, classifiers[k]) for k in classifiers], voting='hard')
ensemble_task1_random.fit(X_train, y_train)
y_pred = ensemble_task1_random.predict(X_test)
results_task1_random['Ensemble'] = {
    'Accuracy': accuracy_score(y_test, y_pred),
    'Precision': precision_score(y_test, y_pred, average='binary'),
    'Recall': recall_score(y_test, y_pred, average='binary'),
    'F1-Score': f1_score(y_test, y_pred, average='binary')
}
print(f"Ensemble Results: Accuracy={results_task1_random['Ensemble']['Accuracy']:.2f}")

#  Task 2: Correlation Analysis with Hyperparameter Tuning (GridSearchCV) 
print("\n Task 2: Correlation Analysis with Hyperparameter Tuning (GridSearchCV)")
plot_correlation_heatmap(X_train, "Correlation Matrix - Task 2 (GridSearchCV)")


# --- Task 3: LDA Feature Reduction with Hyperparameter Tuning (RandomizedSearchCV) ---
print("\n Task 3: LDA Feature Reduction with Hyperparameter Tuning (RandomizedSearchCV) ")
results_task3_random = {}
for name, clf in classifiers.items():
    print(f"\nTuning {name} with RandomizedSearchCV...")
    if param_distributions[name]:
        random_search = RandomizedSearchCV(clf, param_distributions[name], n_iter=10, cv=5, scoring='accuracy', n_jobs=-1, random_state=42)
        random_search.fit(X_train_lda, y_train)
        print(f"Best parameters: {random_search.best_params_}")
        clf = random_search.best_estimator_
    
    clf.fit(X_train_lda, y_train)
    y_pred = clf.predict(X_test_lda)
    results_task3_random[name] = {
        'Accuracy': accuracy_score(y_test, y_pred),
        'Precision': precision_score(y_test, y_pred, average='binary'),
        'Recall': recall_score(y_test, y_pred, average='binary'),
        'F1-Score': f1_score(y_test, y_pred, average='binary')
    }
    print(f"{name} Results: Accuracy={results_task3_random[name]['Accuracy']:.2f}")

ensemble_task3_random = VotingClassifier(estimators=[(k, classifiers[k]) for k in classifiers], voting='hard')
ensemble_task3_random.fit(X_train_lda, y_train)
y_pred = ensemble_task3_random.predict(X_test_lda)
results_task3_random['Ensemble'] = {
    'Accuracy': accuracy_score(y_test, y_pred),
    'Precision': precision_score(y_test, y_pred, average='binary'),
    'Recall': recall_score(y_test, y_pred, average='binary'),
    'F1-Score': f1_score(y_test, y_pred, average='binary')
}
print(f"Ensemble Results: Accuracy={results_task3_random['Ensemble']['Accuracy']:.2f}")

# Visualisation
for task_name, results in [
    ('Task 1: Basic Hyperparameter Tuning (GridSearchCV)', results_task1_grid),
    ('Task 1: Basic Hyperparameter Tuning (RandomizedSearchCV)', results_task1_random),
    ('Task 2: Correlation Analysis with Hyperparameter Tuning (GridSearchCV)', results_task2_grid),
    ('Task 2: Correlation Analysis with Hyperparameter Tuning (RandomizedSearchCV)', results_task2_random),
    ('Task 3: LDA Feature Reduction with Hyperparameter Tuning (GridSearchCV)', results_task3_grid),
    ('Task 3: LDA Feature Reduction with Hyperparameter Tuning (RandomizedSearchCV)', results_task3_random)
]:
    print(f"\nVisualizing {task_name}...")
    models = list(results.keys())
    accuracies = [results[model]['Accuracy'] for model in models]
    
    plt.figure(figsize=(10, 6))
    plt.bar(models, accuracies, color='skyblue')
    plt.title(f'Accuracy Comparison - {task_name}')
    plt.xlabel('Models')
    plt.ylabel('Accuracy')
    plt.xticks(rotation=45)
    plt.ylim(0, 1)
    plt.tight_layout()
    plt.show()




